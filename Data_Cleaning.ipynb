{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Movieslen Dataset analysis using HIVE and Tableau on AWS</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DATA CLEANING</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Movies</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieid|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 7 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "#creating schema for movies dataframe\n",
    "schema4 = StructType([StructField('movieid', IntegerType(), True),StructField('title', StringType(), True),StructField('genres', StringType(), True)])\n",
    "#creating movies dataframe and read csv file from s3 bucket\n",
    "movies = spark.read.csv(\"s3a://project555/movies/movies.csv\", schema=schema4 , header=True)\n",
    "movies.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieid: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+---------+\n",
      "|movieid|           title|   genres|\n",
      "+-------+----------------+---------+\n",
      "|      1|Toy Story (1995)|Adventure|\n",
      "|      1|Toy Story (1995)|Animation|\n",
      "|      1|Toy Story (1995)| Children|\n",
      "|      1|Toy Story (1995)|   Comedy|\n",
      "|      1|Toy Story (1995)|  Fantasy|\n",
      "|      2|  Jumanji (1995)|Adventure|\n",
      "|      2|  Jumanji (1995)| Children|\n",
      "+-------+----------------+---------+\n",
      "only showing top 7 rows"
     ]
    }
   ],
   "source": [
    "#Splitting the genres column of dataframe\n",
    "from pyspark.sql.functions import split, explode\n",
    "movies = movies.withColumn(\"genres\", explode(split(\"genres\", \"[|]\")))\n",
    "movies.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+---------+----+\n",
      "|movieid|           title|   genres|year|\n",
      "+-------+----------------+---------+----+\n",
      "|      1|Toy Story (1995)|Adventure|1995|\n",
      "|      1|Toy Story (1995)|Animation|1995|\n",
      "|      1|Toy Story (1995)| Children|1995|\n",
      "|      1|Toy Story (1995)|   Comedy|1995|\n",
      "|      1|Toy Story (1995)|  Fantasy|1995|\n",
      "|      2|  Jumanji (1995)|Adventure|1995|\n",
      "|      2|  Jumanji (1995)| Children|1995|\n",
      "+-------+----------------+---------+----+\n",
      "only showing top 7 rows"
     ]
    }
   ],
   "source": [
    "#Seperating the year from title column\n",
    "#creating the function insertYear which seperate the year\n",
    "import re\n",
    "def insertYear(title):\n",
    "  year = re.search('\\(([0-9]{4})', title)\n",
    "  if year is not None:\n",
    "    year = year.group(1)\n",
    "  else:\n",
    "    year = 0\n",
    "  return int(year)\n",
    "\n",
    "sqlContext.udf.register(\"yearCleansing\", insertYear)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "#udf used to cast integer datatype to new created 'year' column\n",
    "\n",
    "year_udf = udf(insertYear, IntegerType())\n",
    "df_movie_updated = movies.select(\"movieid\",\"title\",\"genres\",year_udf(\"title\").alias(\"year\"))\n",
    "df_movie_updated.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "#checking Null values in  given dataframe\n",
    "df_movie_updated.filter(df_movie_updated.movieid.isNull()).count()\n",
    "df_movie_updated.filter(df_movie_updated.title.isNull()).count()\n",
    "df_movie_updated.filter(df_movie_updated.genres.isNull()).count()\n",
    "df_movie_updated.filter(df_movie_updated.year.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "#checking '0' values in  given dataframe\n",
    "df_movie_updated.filter(df_movie_updated.movieid==0).count()\n",
    "df_movie_updated.filter(df_movie_updated.title==0).count()\n",
    "df_movie_updated.filter(df_movie_updated.genres==0).count()\n",
    "df_movie_updated.filter(df_movie_updated.year==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the '0' value rows from year column\n",
    "df_movie_updated = df_movie_updated.filter(df_movie_updated.year!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "df_movie_updated.filter(df_movie_updated.year==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing cleaned csv file as ORC format in S3 bucket\n",
    "df_movie_updated.write.format(\"orc\").save(\"s3a://project555/new/movies_clean.orc/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ratings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userid|movieid|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|    307|   3.5|1256677221|\n",
      "|     1|    481|   3.5|1256677456|\n",
      "|     1|   1091|   1.5|1256677471|\n",
      "|     1|   1257|   4.5|1256677460|\n",
      "|     1|   1449|   4.5|1256677264|\n",
      "|     1|   1590|   2.5|1256677236|\n",
      "|     1|   1591|   1.5|1256677475|\n",
      "+------+-------+------+----------+\n",
      "only showing top 7 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "#define schema and create df and read csv from s3 bucket\n",
    "schema5 = (StructType().add(\"userid\", IntegerType()).add(\"movieid\", IntegerType()).add(\"rating\", FloatType()).add(\"timestamp\", LongType()))\n",
    "ratings = spark.read.csv(\"s3a://project555/ratings/ratings.csv\", schema=schema5, header=True)\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "#functions func is used to round the rating column\n",
    "ratings = ratings.withColumn(\"rating\", func.round(ratings[\"rating\"], 1))\n",
    "ratings.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "#checking Null values in given dataframe\n",
    "ratings.filter(ratings.userid.isNull()).count()\n",
    "ratings.filter(ratings.movieid.isNull()).count()\n",
    "ratings.filter(ratings.rating.isNull()).count()\n",
    "ratings.filter(ratings.timestamp.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "#checking '0' values in  given dataframe\n",
    "ratings.filter(ratings.userid==0).count()\n",
    "ratings.filter(ratings.movieid==0).count()\n",
    "ratings.filter(ratings.rating==0).count()\n",
    "ratings.filter(ratings.timestamp==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+-------------------+\n",
      "|userid|movieid|rating| timestamp|           Datetime|\n",
      "+------+-------+------+----------+-------------------+\n",
      "|     1|    307|   3.5|1256677221|2009-10-27 21:00:21|\n",
      "|     1|    481|   3.5|1256677456|2009-10-27 21:04:16|\n",
      "|     1|   1091|   1.5|1256677471|2009-10-27 21:04:31|\n",
      "|     1|   1257|   4.5|1256677460|2009-10-27 21:04:20|\n",
      "|     1|   1449|   4.5|1256677264|2009-10-27 21:01:04|\n",
      "|     1|   1590|   2.5|1256677236|2009-10-27 21:00:36|\n",
      "|     1|   1591|   1.5|1256677475|2009-10-27 21:04:35|\n",
      "+------+-------+------+----------+-------------------+\n",
      "only showing top 7 rows"
     ]
    }
   ],
   "source": [
    "#Convert the timestamp column from df into DateTime \n",
    "from pyspark.sql import functions as func \n",
    "ratings= ratings.withColumn('Datetime', func.from_unixtime('timestamp').cast(TimestampType()))\n",
    "ratings.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userid|movieid|rating|      date|\n",
      "+------+-------+------+----------+\n",
      "|     1|    307|   3.5|2009-10-27|\n",
      "|     1|    481|   3.5|2009-10-27|\n",
      "|     1|   1091|   1.5|2009-10-27|\n",
      "|     1|   1257|   4.5|2009-10-27|\n",
      "|     1|   1449|   4.5|2009-10-27|\n",
      "|     1|   1590|   2.5|2009-10-27|\n",
      "|     1|   1591|   1.5|2009-10-27|\n",
      "+------+-------+------+----------+\n",
      "only showing top 7 rows"
     ]
    }
   ],
   "source": [
    "##splitting the Datetime column into Date and Time\n",
    "import pyspark.sql.functions as F\n",
    "ratings_split = F.split(ratings['Datetime'], ' ')\n",
    "ratings = ratings.withColumn('date', ratings_split.getItem(0))\n",
    "##ratings = ratings.withColumn('time', ratings_split.getItem(1))\n",
    "ratings = ratings.drop('timestamp','Datetime')\n",
    "ratings.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing cleaned csv file as ORC format in S3 bucket\n",
    "ratings.write.format(\"orc\").save(\"s3a://project555/new/ratings_clean.orc/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tags</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+----------+\n",
      "|userid|movieid|         tag| timestamp|\n",
      "+------+-------+------------+----------+\n",
      "|    14|    110|        epic|1443148538|\n",
      "|    14|    110|    Medieval|1443148532|\n",
      "|    14|    260|      sci-fi|1442169410|\n",
      "|    14|    260|space action|1442169421|\n",
      "|    14|    318|imdb top 250|1442615195|\n",
      "|    14|    318|     justice|1442615192|\n",
      "|    14|    480|   Dinosaurs|1443148563|\n",
      "+------+-------+------------+----------+\n",
      "only showing top 7 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "#define schema and create df and read csv from s3 bucket\n",
    "schema6 = (StructType().add(\"userid\", IntegerType()).add(\"movieid\", IntegerType()).add(\"tag\", StringType()).add(\"timestamp\", LongType()))\n",
    "tags = spark.read.csv(\"s3a://project555/tags/tags.csv\", schema=schema6, header=True)\n",
    "tags.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+----------+-------------------+\n",
      "|userid|movieid|         tag| timestamp|           Datetime|\n",
      "+------+-------+------------+----------+-------------------+\n",
      "|    14|    110|        epic|1443148538|2015-09-25 02:35:38|\n",
      "|    14|    110|    Medieval|1443148532|2015-09-25 02:35:32|\n",
      "|    14|    260|      sci-fi|1442169410|2015-09-13 18:36:50|\n",
      "|    14|    260|space action|1442169421|2015-09-13 18:37:01|\n",
      "|    14|    318|imdb top 250|1442615195|2015-09-18 22:26:35|\n",
      "|    14|    318|     justice|1442615192|2015-09-18 22:26:32|\n",
      "|    14|    480|   Dinosaurs|1443148563|2015-09-25 02:36:03|\n",
      "+------+-------+------------+----------+-------------------+\n",
      "only showing top 7 rows"
     ]
    }
   ],
   "source": [
    "#Convert the timestamp column from df into DateTime \n",
    "from pyspark.sql import functions as func \n",
    "tag= tags.withColumn('Datetime', func.from_unixtime('timestamp').cast(TimestampType()))\n",
    "tag.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "#checking Null values in given dataframe\n",
    "tag.filter(tag.userid.isNull()).count()\n",
    "tag.filter(tag.movieid.isNull()).count()\n",
    "tag.filter(tag.tag.isNull()).count()\n",
    "tag.filter(tag.timestamp.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "#droping Null values from timestamp column\n",
    "tag = tag.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "tag.filter(tag.timestamp.isNull()).count()\n",
    "tag.filter(tag.Datetime.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+----------+\n",
      "|userid|movieid|         tag|      date|\n",
      "+------+-------+------------+----------+\n",
      "|    14|    110|        epic|2015-09-25|\n",
      "|    14|    110|    Medieval|2015-09-25|\n",
      "|    14|    260|      sci-fi|2015-09-13|\n",
      "|    14|    260|space action|2015-09-13|\n",
      "|    14|    318|imdb top 250|2015-09-18|\n",
      "|    14|    318|     justice|2015-09-18|\n",
      "|    14|    480|   Dinosaurs|2015-09-25|\n",
      "+------+-------+------------+----------+\n",
      "only showing top 7 rows"
     ]
    }
   ],
   "source": [
    "##splitting the Datetime column into Date and Time\n",
    "import pyspark.sql.functions as F\n",
    "tagclean = F.split(tag['Datetime'], ' ')\n",
    "tag = tag.withColumn('date', tagclean.getItem(0))\n",
    "##tag= tag.withColumn('time', tagclean.getItem(1))\n",
    "\n",
    "##Drop the timestamp and Datetime Column\n",
    "tag_clean = tag.drop('timestamp','Datetime')\n",
    "tag_clean.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing cleaned csv file as ORC format in S3 bucket\n",
    "tag_clean.write.format(\"orc\").save(\"s3a://project555/new/tags_clean.orc/\")\n",
    "\n",
    "##Read the multiple file from s3 bucket, give the path of folder contain multiple files\n",
    "#dff = sqlContext.read.format('orc').load(\"s3a://project555/new_cleaning/links_clean.orc/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Links</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|movieid|imdbid|tmdbid|\n",
      "+-------+------+------+\n",
      "|      1|114709|   862|\n",
      "|      2|113497|  8844|\n",
      "|      3|113228| 15602|\n",
      "|      4|114885| 31357|\n",
      "|      5|113041| 11862|\n",
      "|      6|113277|   949|\n",
      "|      7|114319| 11860|\n",
      "+-------+------+------+\n",
      "only showing top 7 rows"
     ]
    }
   ],
   "source": [
    "#create a schema in spark for link file\n",
    "from pyspark.sql.types import *\n",
    "#define schema and create df and read csv from s3 bucket\n",
    "schema1 = StructType([StructField('movieid', IntegerType(), True),StructField('imdbid',  IntegerType(), True),StructField('tmdbid', IntegerType(), True)])\n",
    "\n",
    "#read data from s3 bucket\n",
    "link = spark.read.csv(\"s3a://project555/links/links.csv\", schema=schema1 , header=True)\n",
    "link.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "#checking Null values in  given dataframe\n",
    "\n",
    "link.filter(link.movieid.isNull()).count()\n",
    "link.filter(link.imdbid.isNull()).count()\n",
    "link.filter(link.tmdbid.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "#cleaning Null values from tmdbId Column\n",
    "\n",
    "link = link.dropna(subset=[\"tmdbid\"])\n",
    "link.filter(link.tmdbid.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "#checking '0' values in  given dataframe\n",
    "link.filter(link.movieid==0).count()\n",
    "link.filter(link.imdbid==0).count()\n",
    "link.filter(link.tmdbid==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing cleaned csv file as ORC format in S3 bucket\n",
    "link.write.format(\"orc\").save(\"s3a://project555/new/links_clean.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Genome-tags</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|tagid|         tag|\n",
      "+-----+------------+\n",
      "|    1|         007|\n",
      "|    2|007 (series)|\n",
      "|    3|18th century|\n",
      "|    4|       1920s|\n",
      "|    5|       1930s|\n",
      "|    6|       1950s|\n",
      "|    7|       1960s|\n",
      "+-----+------------+\n",
      "only showing top 7 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "#define schema and create df and read csv from s3 bucket\n",
    "schema3 = StructType([StructField('tagid', IntegerType(), True),StructField('tag', StringType(), True)])\n",
    "\n",
    "genome_tags = spark.read.csv(\"s3a://project555/genome-tags/genome-tags.csv\", schema=schema3, header=True)\n",
    "genome_tags.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "#checking Null values in given dataframe\n",
    "genome_tags.filter(genome_tags.tagid.isNull()).count()\n",
    "genome_tags.filter(genome_tags.tag.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "#checking '0' values in given dataframe\n",
    "genome_tags.filter(genome_tags.tagid==0).count()\n",
    "genome_tags.filter(genome_tags.tag==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing cleaned csv file as ORC format in S3 bucket\n",
    "genome_tags.write.format(\"orc\").save(\"s3a://project555/new/genome_tags_clean.orc/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Genome-scores</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+\n",
      "|movieid|tagid|relevance|\n",
      "+-------+-----+---------+\n",
      "|      1|    1|    0.029|\n",
      "|      1|    2|    0.024|\n",
      "|      1|    3|    0.054|\n",
      "|      1|    4|    0.069|\n",
      "|      1|    5|     0.16|\n",
      "|      1|    6|    0.195|\n",
      "|      1|    7|    0.076|\n",
      "+-------+-----+---------+\n",
      "only showing top 7 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "#define schema and create df and read csv from s3 bucket\n",
    "schema2 = StructType([StructField('movieid', IntegerType(), True),StructField('tagid',  IntegerType(), True),StructField('relevance', FloatType(), True)])\n",
    "\n",
    "genome_scores = spark.read.csv(\"s3a://project555/genome-scores/genome-scores.csv\", schema=schema2 , header=True)\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "#functions func is used to round the relevance column\n",
    "genome_scores = genome_scores.withColumn(\"relevance\", func.round(genome_scores[\"relevance\"], 3))\n",
    "genome_scores.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "#checking Null values in given dataframe\n",
    "genome_scores.filter(genome_scores.movieid.isNull()).count()\n",
    "genome_scores.filter(genome_scores.tagid.isNull()).count()\n",
    "genome_scores.filter(genome_scores.relevance.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10"
     ]
    }
   ],
   "source": [
    "#data cleaning\n",
    "#checking '0' values in given dataframe\n",
    "genome_scores.filter(genome_scores.movieid==0).count()\n",
    "genome_scores.filter(genome_scores.tagid==0).count()\n",
    "genome_scores.filter(genome_scores.relevance==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_scores = genome_scores.filter(genome_scores.relevance!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "genome_scores.filter(genome_scores.relevance==0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing cleaned csv file as ORC format in S3 bucket\n",
    "genome_scores.write.format(\"orc\").save(\"s3a://project555/new/genome_scores_clean.orc/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
